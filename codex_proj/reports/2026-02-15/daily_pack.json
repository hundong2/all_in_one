{
  "date": "2026-02-15",
  "cards": [
    {
      "paper_id": "arxiv_2406_openvla",
      "paper_title": "OpenVLA: An Open-Source Vision-Language-Action Model",
      "domain": "VLM",
      "summary": "1) OpenVLA unifies vision tokens and language instructions into one policy stack for robot action prediction. 2) It demonstrates open replication paths compared with closed VLA systems and supports fine-tuning for new tasks. 3) The core gain is practical transfer from web-scale priors to embodied manipulation with less task-specific engineering.",
      "novelty": "Open-source VLA baseline with end-to-end multimodal action decoding and practical adaptation path.",
      "key_method_diagram_text": "RGB image -> Vision encoder -> Visual tokens; Instruction text -> Text encoder -> Text tokens; Token fusion -> Action head -> Continuous robot action",
      "sota_delta": "N/A in this local run (paper-reported benchmark deltas not re-evaluated).",
      "failure_modes": [
        "Instruction ambiguity causes unstable action outputs in cluttered scenes.",
        "Out-of-distribution objects degrade grasp success due to weak visual grounding.",
        "Latency increases on edge hardware, reducing closed-loop stability."
      ],
      "repro_checklist": [
        "Pin Python + PyTorch versions",
        "Use deterministic seed",
        "Validate dataset schema (image, token_ids, action)",
        "Single-batch forward before long run"
      ],
      "execution_template_path": "templates/openvla",
      "smoke_log_path": "logs/openvla_smoke.log",
      "smoke_status": "PASS",
      "smoke_metrics": {
        "loss": 0.705593
      }
    },
    {
      "paper_id": "rt2_2023",
      "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "domain": "Robotics",
      "summary": "1) RT-2 casts robotic control as token prediction conditioned on image and language context. 2) It injects knowledge from web-scale vision-language pretraining into manipulation behavior. 3) The approach improves semantic generalization for long-tail instructions.",
      "novelty": "Tokenized action generation from VLM representations for embodied control.",
      "key_method_diagram_text": "Image + instruction -> Shared VLM backbone -> Token decoder -> Action token -> Robot low-level controller",
      "sota_delta": "N/A in this local run (paper-reported benchmark deltas not re-evaluated).",
      "failure_modes": [
        "Token decoding errors accumulate over long-horizon tasks.",
        "Camera viewpoint shifts can break instruction grounding.",
        "Mismatch between action token vocabulary and hardware interface causes brittle execution."
      ],
      "repro_checklist": [
        "Confirm action-token mapping",
        "Freeze backbone for initial sanity checks",
        "Validate instruction tokenization path",
        "Run single-step imitation loss check"
      ],
      "execution_template_path": "templates/rt2",
      "smoke_log_path": "logs/rt2_smoke.log",
      "smoke_status": "PASS",
      "smoke_metrics": {
        "loss": 5.607576
      }
    },
    {
      "paper_id": "diffusion_policy_2023",
      "paper_title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
      "domain": "Physical AI",
      "summary": "1) Diffusion Policy models action generation as denoising over trajectory chunks. 2) It supports multimodal behavior and smoother long-horizon control by iterative refinement. 3) The formulation often improves robustness to noisy demonstrations compared with direct regression.",
      "novelty": "Action denoising objective for visuomotor policy instead of one-shot action regression.",
      "key_method_diagram_text": "State trajectory + noisy actions + timestep -> Denoiser network -> Predicted noise -> Iterative denoise -> Executable action chunk",
      "sota_delta": "N/A in this local run (paper-reported benchmark deltas not re-evaluated).",
      "failure_modes": [
        "Insufficient denoising steps underfit multimodal action distribution.",
        "Schedule mismatch between training and inference timesteps destabilizes rollout.",
        "Low-quality demonstrations can induce mode collapse in action proposals."
      ],
      "repro_checklist": [
        "Check state/action tensor shapes",
        "Verify noise schedule and timestep normalization",
        "Start with short horizon smoke run",
        "Track MSE against sampled noise"
      ],
      "execution_template_path": "templates/diffusion_policy",
      "smoke_log_path": "logs/diffusion_policy_smoke.log",
      "smoke_status": "PASS",
      "smoke_metrics": {
        "loss": 1.118454
      }
    }
  ],
  "action_items": [
    "Add orchestrator preflight dependency checks.",
    "Integrate `scripts/update_daily_pack_smoke.py` into daily pipeline after Validator.",
    "Optionally add CI smoke-test job for template regressions."
  ]
}
